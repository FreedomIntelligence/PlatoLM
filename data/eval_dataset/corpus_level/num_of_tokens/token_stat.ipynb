{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mntcephfs/lab_data/fanyaxin/miniconda3/envs/LLAMA2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict: Dict,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict) \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.42s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_tokenizer(path='/mntcephfs/data/med/zhihong/workspace/LLMZoo/llama_hf_7b'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        path, \n",
    "        model_max_length=2048, \n",
    "        padding_side=\"right\", \n",
    "        use_fast=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    })\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "\n",
    "def extract_utters(path):\n",
    "    all_utter = []\n",
    "    session_list = read_json_file(path)\n",
    "    for session_dict in session_list:\n",
    "        for utter_dict in session_dict['conversations']:\n",
    "            all_utter.append(utter_dict['value'])\n",
    "    return all_utter\n",
    "\n",
    "def compute_avg_utters(path):\n",
    "    '''所有utter的token的length / 所有utter的数量'''\n",
    "    utter_len = []\n",
    "    all_utter = extract_utters(path)\n",
    "    for utter in tqdm(all_utter):\n",
    "        idx_tensor = tokenizer(utter, return_tensors=\"pt\", padding=\"longest\")['input_ids'][0]\n",
    "        utter_len.append(len(idx_tensor)-1)\n",
    "    return np.mean(utter_len)\n",
    "\n",
    "def extract_conv(path):\n",
    "    all_conv = []\n",
    "    session_list = read_json_file(path)\n",
    "    for session_dict in session_list:\n",
    "        one_conv = ''\n",
    "        for utter_dict in session_dict['conversations']:\n",
    "            one_conv += (utter_dict['value'])\n",
    "        all_conv.append(one_conv) \n",
    "    return all_conv\n",
    "\n",
    "def compute_avg_convs(path):\n",
    "    '''1session内所有utter的token的length / 多少个session'''\n",
    "    all_conv = extract_conv(path)\n",
    "    one_conv_len = []\n",
    "    for one_conv_str in tqdm(all_conv):\n",
    "        idx_tensor = tokenizer(one_conv_str, return_tensors=\"pt\", padding=\"longest\")['input_ids'][0]\n",
    "        one_conv_len.append(len(idx_tensor)-1)\n",
    "    # assert len(one_conv_len) == 10000\n",
    "    return np.mean(one_conv_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75746/75746 [01:27<00:00, 861.66it/s]\n",
      "100%|██████████| 10000/10000 [01:59<00:00, 83.91it/s]\n",
      " 25%|██▌       | 1/4 [03:30<10:31, 210.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/hei/ww_10k.json:\n",
      "avg_utter_len_by_token:  278.812135294273\n",
      "avg_conv_len_by_token:  2112.2579\n",
      "Elapsed 210.4267475605011 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234356/234356 [04:15<00:00, 915.88it/s]\n",
      "100%|██████████| 20039/20039 [13:53<00:00, 24.04it/s]\n",
      " 50%|█████     | 2/4 [21:48<24:24, 732.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/hei/sg_20039_not_splitted_for_eval.json:\n",
      "avg_utter_len_by_token:  207.1990902729181\n",
      "avg_conv_len_by_token:  2424.737062727681\n",
      "Elapsed 1097.6068377494812 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215378/215378 [03:44<00:00, 959.33it/s]\n",
      "100%|██████████| 20039/20039 [03:58<00:00, 84.18it/s]\n",
      " 75%|███████▌  | 3/4 [29:38<10:12, 612.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/hei/sd_20039_for_eval.json:\n",
      "avg_utter_len_by_token:  203.13285479482585\n",
      "avg_conv_len_by_token:  2183.662907330705\n",
      "Elapsed 470.1716182231903 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166078/166078 [01:45<00:00, 1567.66it/s]\n",
      "100%|██████████| 10000/10000 [01:37<00:00, 102.87it/s]\n",
      "100%|██████████| 4/4 [33:05<00:00, 496.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/hei/dw_10k.json:\n",
      "avg_utter_len_by_token:  120.52562651284336\n",
      "avg_conv_len_by_token:  2003.1139\n",
      "Elapsed 206.98335146903992 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = r\"/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/hei\"\n",
    "\n",
    "files_name = []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        files_name.append(os.path.join(root, file))\n",
    "\n",
    "for file in tqdm(files_name):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_utter_len = compute_avg_utters(file)\n",
    "    avg_conv_len = compute_avg_convs(file)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'\\nfor{file}:')\n",
    "    print('avg_utter_len_by_token: ', avg_utter_len)\n",
    "    print('avg_conv_len_by_token: ', avg_conv_len)\n",
    "    print(f'Elapsed {end-start} seconds.\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 61354/61354 [01:32<00:00, 666.78it/s]\n",
      "100%|██████████| 10000/10000 [01:39<00:00, 100.13it/s]\n",
      " 17%|█▋        | 1/6 [03:15<16:16, 195.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/vicuna_10k_for_train.json:\n",
      "avg_utter_len_by_token:  185.15452945203248\n",
      "avg_conv_len_by_token:  1136.7103\n",
      "Elapsed 195.3479278087616 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112024/112024 [02:48<00:00, 665.64it/s]\n",
      "100%|██████████| 10000/10000 [02:53<00:00, 57.61it/s]\n",
      " 33%|███▎      | 2/6 [09:03<19:00, 285.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/cwb_raw_10k_duplicate_4096.json:\n",
      "avg_utter_len_by_token:  177.83426765693065\n",
      "avg_conv_len_by_token:  1992.7417\n",
      "Elapsed 348.16182708740234 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75790/75790 [00:38<00:00, 1987.47it/s]\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 486.94it/s]\n",
      " 50%|█████     | 3/6 [10:03<09:07, 182.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/baize_10k_for_train.json:\n",
      "avg_utter_len_by_token:  34.56260720411664\n",
      "avg_conv_len_by_token:  263.122\n",
      "Elapsed 60.259963274002075 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87654/87654 [02:52<00:00, 508.83it/s]\n",
      "100%|██████████| 10000/10000 [03:20<00:00, 49.77it/s]\n",
      " 67%|██████▋   | 4/6 [16:24<08:41, 260.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/s_10k.json:\n",
      "avg_utter_len_by_token:  252.8585575102106\n",
      "avg_conv_len_by_token:  2216.8345\n",
      "Elapsed 380.32848501205444 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107754/107754 [02:55<00:00, 612.86it/s]\n",
      "100%|██████████| 10000/10000 [03:25<00:00, 48.74it/s]\n",
      " 83%|████████▎ | 5/6 [22:51<05:06, 306.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/s_raw_10k_duplicate_3146.json:\n",
      "avg_utter_len_by_token:  202.54971509178313\n",
      "avg_conv_len_by_token:  2182.9382\n",
      "Elapsed 387.37149453163147 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76958/76958 [02:04<00:00, 620.61it/s]\n",
      "100%|██████████| 10000/10000 [02:11<00:00, 75.87it/s]\n",
      "100%|██████████| 6/6 [27:10<00:00, 271.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for/mntcephfs/lab_data/kongchuyi/s2/fastchat/data/ultra_10k_for_train.json:\n",
      "avg_utter_len_by_token:  187.24171626081758\n",
      "avg_conv_len_by_token:  1441.9932\n",
      "Elapsed 259.42885088920593 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = r\"/mntcephfs/lab_data/kongchuyi/s2/fastchat/data\"  \n",
    "files_name = []\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if \"10k\" in file:\n",
    "            files_name.append(os.path.join(root, file))\n",
    "            \n",
    "for file in tqdm(files_name):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_utter_len = compute_avg_utters(file)\n",
    "    avg_conv_len = compute_avg_convs(file)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'\\nfor{file}:')\n",
    "    print('avg_utter_len_by_token: ', avg_utter_len)\n",
    "    print('avg_conv_len_by_token: ', avg_conv_len)\n",
    "    print(f'Elapsed {end-start} seconds.\\n')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
